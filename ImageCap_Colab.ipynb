{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmoTol0L6W8J"
      },
      "source": [
        "Image caption Generator = understanding image and a language description for that image. \n",
        "\n",
        "Sound Interesting? Let’s Begin!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86V1WO806W8N"
      },
      "source": [
        "# Approach\n",
        "\n",
        "We will tackle this problem using an Encoder-Decoder model. Here our encoder model will combine both the encoded form of the image and the encoded form of the text caption and feed to the decoder.\n",
        "Our model will treat CNN as the ‘image model’ and the RNN/LSTM as the ‘language model’ to encode the text sequences of varying length. The vectors resulting from both the encodings are then merged and processed by a Dense layer to make a final prediction.\n",
        "\n",
        "To encode our image features we will make use of transfer learning. There are a lot of models that we can use like VGG-16, InceptionV3, ResNet, etc.\n",
        "\n",
        "To encode our text sequence we will map every word to a 200-dimensional vector. For this will use a pre-trained Glove model. This mapping will be done in a separate layer after the input layer called the embedding layer.\n",
        "To generate the caption we will be using two popular methods which are Greedy Search and Beam Search. These methods will help us in picking the best words to accurately define the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dP5lBH7p6W8O"
      },
      "source": [
        "# Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LL_dgyEg6W8O"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import glob\n",
        "\n",
        "from os import listdir\n",
        "from pickle import dump, load\n",
        "\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from tqdm.notebook import tqdm\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.applications.imagenet_utils import decode_predictions\n",
        "from tensorflow.keras.preprocessing import image, sequence\n",
        "\n",
        "import matplotlib as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjkNqRcc8Z-M",
        "outputId": "aa0c2b9e-e5c7-485a-b826-6d17691586be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT86JDuV6W8P"
      },
      "source": [
        "# Prepare Photo Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yZ32hz36W8P"
      },
      "source": [
        "In the Flickr8k dataset, each image is associated with five different captions that describe the entities and events depicted in the image that were collected. By associating each image with multiple, independently produced sentences, the dataset captures some of the linguistic variety that can be used to describe the same image.\n",
        "\n",
        "Flickr8k is a good starting dataset as it is small in size and can be trained easily on low-end laptops/desktops using a CPU.\n",
        "Our dataset structure is as follows:-\n",
        "\n",
        "- Flick8k/\n",
        "    - Flick8k_Dataset/ :- contains the 8000 images\n",
        "    - Flick8k_Text/\n",
        "        - Flickr8k.token.txt:- contains the image id along with the 5 captions\n",
        "        - Flickr8k.trainImages.txt:- contains the training image id’s\n",
        "        - Flickr8k.testImages.txt:- contains the test image id’s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WIG5uo8W6W8Q"
      },
      "outputs": [],
      "source": [
        "# Defining the directory we are using\n",
        "directory = '/content/drive/My Drive/Kaggle/ImageCaptioningGenerator/input/flickr8k_dataset/Flicker8k_Dataset'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgNOAtQy6W8Q"
      },
      "source": [
        "# what feature image extractor model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfIXnwC56W8R",
        "outputId": "0a81737b-5f94-46e1-e74f-281a9e27f00f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467096/553467096 [==============================] - 20s 0us/step\n"
          ]
        }
      ],
      "source": [
        "model = VGG16()\n",
        "\n",
        "# Removing the last layer from the loaded model as we require only the features not the classification \n",
        "model.layers.pop()\n",
        "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
        "\n",
        "#Total params: 134,260,544"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eIhs_rj6W8R",
        "outputId": "2fe86ca1-1a9d-409e-f841-225c510b02b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
            "96112376/96112376 [==============================] - 4s 0us/step\n"
          ]
        }
      ],
      "source": [
        "model = InceptionV3(weights='imagenet')\n",
        "model = Model(model.input, model.layers[-2].output)\n",
        "\n",
        "#Total params: 21,802,784"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5a1C_tx6W8S",
        "outputId": "cdbd5841-5d34-454f-d74f-306b9399e371"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 4s 0us/step\n"
          ]
        }
      ],
      "source": [
        "model = ResNet50(include_top=False,weights='imagenet',input_shape=(224,224,3),pooling='avg')\n",
        "#Total params: 23,587,712"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32bHDR7J6W8S"
      },
      "source": [
        "* We will choose RestNet50 model: good performance and less variable\n",
        "* Since we are using ResNet50 we need to pre-process our input before feeding it into the model. Hence we define a preprocess function to reshape the images to (224 x 224) and feed to the preprocess_input() function of Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7JvL5if-6W8S"
      },
      "outputs": [],
      "source": [
        "def preprocessing(img_path) :\n",
        "    # Loading an image and converting it into size 224 * 224\n",
        "    im = image.load_img(img_path, target_size=(224,224,3))\n",
        "    # Converting the image pixels into a numpy array\n",
        "    im = image.img_to_array(im)\n",
        "    # Reshaping data for the model\n",
        "    im = im.reshape((1, im.shape[0], im.shape[1], im.shape[2]))\n",
        "    return im"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pbKUL5cm6W8T"
      },
      "outputs": [],
      "source": [
        "# Extract features from each photo in the directory\n",
        "def extract_features(directory):\n",
        "\n",
        "    # Extracting features from each photo and storing it in a dictionary \n",
        "    features = dict()\n",
        "\n",
        "    for name in tqdm(listdir(directory)):\n",
        "\n",
        "        # Defining the path of the image \n",
        "        filename = directory + '/' + name\n",
        "        \n",
        "        # Loading an image and converting it into size 224 * 224\n",
        "        image = preprocessing(filename)\n",
        "\n",
        "        # Preprocessing the images for the model\n",
        "        # The preprocess_input function is meant to adequate your image to the format the model requires.\n",
        "        image = preprocess_input(image)\n",
        "\n",
        "        # Getting features of an image\n",
        "        feature = model.predict(image, verbose=0)\n",
        "        \n",
        "        # Getting the image name\n",
        "        image_id = name.split('.')[0]\n",
        "\n",
        "        # Storing the feature corresponding to the image in the dictionary\n",
        "        features[image_id] = feature\n",
        "        \n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xIe5CA156W8T"
      },
      "outputs": [],
      "source": [
        "#features = extract_features(directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Vg9A-QPi6W8U"
      },
      "outputs": [],
      "source": [
        "# store the features\n",
        "#dump(features, open('features.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA54WEY16W8U"
      },
      "source": [
        "2. Preparing Text Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "mU7RbUPj6W8U"
      },
      "outputs": [],
      "source": [
        "# Loading the file containing all the descriptions into memory\n",
        "\n",
        "def load_doc(filename):\n",
        "    #Opening the file as read only\n",
        "    file = open(filename, 'r')\n",
        "\n",
        "    #Reading all text and storing it.\n",
        "    text = file.read()\n",
        "\n",
        "    #Closing the file\n",
        "    file.close()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3NFEEerE6W8U"
      },
      "outputs": [],
      "source": [
        "def photo_to_description_mapping(descriptions):\n",
        "    \n",
        "    # Dictionary to store the mapping of photo identifiers to descriptions\n",
        "    description_mapping = dict()\n",
        "    \n",
        "    # Iterating through each line of the descriptions\n",
        "    for line in descriptions.split('\\n'):\n",
        "        \n",
        "        # Splitting the lines by white space\n",
        "        words = line.split()\n",
        "        \n",
        "        # Skipping the lines with length less than 2\n",
        "        if len(line)<2:\n",
        "            continue\n",
        "        \n",
        "        # The first word is the image_id and the rest are the part of the description of that image\n",
        "        image_id, image_description = words[0], words[1:]\n",
        "        \n",
        "        # Retaining only the name of the image and removing the extension from it\n",
        "        image_id = image_id.split('.')[0]\n",
        "        \n",
        "        # Image_descriptions contains comma separated words of the description, hence, converting it back to string\n",
        "        image_description = ' '.join(image_description)\n",
        "        \n",
        "        # There are multiple descriptions per image, \n",
        "        # hence, corresponding to every image identifier in the dictionary, there is a list of description\n",
        "        # if the list does not exist then we need to create it\n",
        "        \n",
        "        if image_id not in description_mapping:\n",
        "            description_mapping[image_id] = list()\n",
        "        \n",
        "        # Now storing the descriptions in the mapping\n",
        "        description_mapping[image_id].append(image_description)\n",
        "    \n",
        "    return description_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4qeETg7Y6W8V"
      },
      "outputs": [],
      "source": [
        "def clean_descriptions(description_mapping):\n",
        "    \n",
        "    # Preapring a translation table for removing all the punctuation\n",
        "    table = str.maketrans('','', string.punctuation)\n",
        "    # Traversing through the mapping we created\n",
        "    for key, descriptions in description_mapping.items():\n",
        "        for i in range(len(descriptions)):\n",
        "            description = descriptions[i]\n",
        "            description = description.split()\n",
        "            \n",
        "            # Converting all the words to lower case\n",
        "            description = [word.lower() for word in description]\n",
        "            \n",
        "            # Removing the punctuation using the translation table we made\n",
        "            description = [word.translate(table) for word in description]\n",
        "            \n",
        "            # Removing the words with length =1\n",
        "            description = [word for word in description if len(word)>1]\n",
        "            \n",
        "            # Removing all words with number in them\n",
        "            description = [word for word in description if word.isalpha()]\n",
        "\n",
        "            # Converting the description back to string and overwriting in the descriptions list\n",
        "            descriptions[i] = ' '.join(description)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME7Pu-xU6W8V"
      },
      "source": [
        "Ideally, we want a vocabulary that is both expressive and as small as possible. A smaller vocabulary will result in a smaller model that will train faster.\n",
        "\n",
        "For reference, we can transform the clean descriptions into a set and print its size to get an idea of the size of our dataset vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "QW6PPcf_6W8V"
      },
      "outputs": [],
      "source": [
        "# Converting the loaded descriptions into a vocabulary of words\n",
        "\n",
        "def to_vocabulary(descriptions):\n",
        "    \n",
        "    # Build a list of all description strings\n",
        "    all_desc = set()\n",
        "    \n",
        "    for key in descriptions.keys():\n",
        "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
        "    \n",
        "    return all_desc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "tt8t_Fjw6W8V"
      },
      "outputs": [],
      "source": [
        "# save descriptions to file, one per line\n",
        "def save_descriptions(descriptions, filename):\n",
        "    lines = list()\n",
        "    for key, desc_list in descriptions.items():\n",
        "        for desc in desc_list:\n",
        "            lines.append(key + ' ' + desc)\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGbqTS976W8W",
        "outputId": "96b2514c-9224-4f4a-d318-1356fd67a479"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: 8092 \n",
            "Vocabulary Size: 8763\n"
          ]
        }
      ],
      "source": [
        "filename = \"/content/drive/My Drive/Kaggle/ImageCaptioningGenerator/input/Flickr8k_text/Flickr8k.token.txt\"\n",
        "\n",
        "# Loading descriptions\n",
        "doc = load_doc(filename)\n",
        "\n",
        "# Parsing descriptions\n",
        "descriptions = photo_to_description_mapping(doc)\n",
        "print('Loaded: %d ' % len(descriptions))\n",
        "\n",
        "# Cleaning the descriptions\n",
        "clean_descriptions(descriptions)\n",
        "\n",
        "# Summarizing the vocabulary\n",
        "vocabulary = to_vocabulary(descriptions)\n",
        "print('Vocabulary Size: %d' % len(vocabulary))\n",
        "\n",
        "# Saving to the file\n",
        "save_descriptions(descriptions, 'descriptions.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIklpvF86W8W"
      },
      "source": [
        "Next, we create a vocabulary of all the unique words present across all the 8000*5 (i.e. 40000) image captions in the data set. We have 8763 unique words across all the 40000 image captions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "37tgeiAx6W8W"
      },
      "outputs": [],
      "source": [
        "# Function for loading a file into memory and returning text from it\n",
        "def load_file(filename):\n",
        "    file = open(filename, 'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# Function for loading a pre-defined list of photo identifiers\n",
        "def load_photo_identifiers(filename):\n",
        "    \n",
        "    # Loading the file containing the list of photo identifier\n",
        "    file = load_file(filename)\n",
        "    \n",
        "    # Creating a list for storing the identifiers\n",
        "    photos = list()\n",
        "    \n",
        "    # Traversing the file one line at a time\n",
        "    for line in file.split('\\n'):\n",
        "        if len(line) < 1:\n",
        "            continue\n",
        "        \n",
        "        # Image name contains the extension as well but we need just the name\n",
        "        identifier = line.split('.')[0]\n",
        "\n",
        "        # Adding it to the list of photos\n",
        "        photos.append(identifier)\n",
        "        \n",
        "    # Returning the set of photos created\n",
        "    return set(photos)\n",
        "\n",
        "\n",
        "# loading the cleaned descriptions that we created earlier\n",
        "# we will only be loading the descriptions of the images that we will use for training\n",
        "# hence we need to pass the set of train photos that the above function will be returning\n",
        "\n",
        "def load_clean_descriptions(filename, photos):\n",
        "    \n",
        "    #loading the cleaned description file\n",
        "    file = load_file(filename)\n",
        "    \n",
        "    #creating a dictionary of descripitions for storing the photo to description mapping of train images\n",
        "    descriptions = dict()\n",
        "    \n",
        "    #traversing the file line by line\n",
        "    for line in file.split('\\n'):\n",
        "        # splitting the line at white spaces\n",
        "        words = line.split()\n",
        "\n",
        "        # the first word will be the image name and the rest will be the description of that particular image\n",
        "        image_id, image_description = words[0], words[1:]\n",
        "        \n",
        "        # we want to load only those description which corresponds to the set of photos we provided as argument\n",
        "        if image_id in photos:\n",
        "            #creating list of description if needed\n",
        "            if image_id not in descriptions:\n",
        "                descriptions[image_id] = list()\n",
        "            \n",
        "            #the model we will develop will generate a caption given a photo, \n",
        "            #and the caption will be generated one word at a time. \n",
        "            #The sequence of previously generated words will be provided as input. \n",
        "            #Therefore, we will need a ‘first word’ to kick-off the generation process \n",
        "            #and a ‘last word‘ to signal the end of the caption.\n",
        "            #we will use 'startseq' and 'endseq' for this purpose\n",
        "            #also we have to convert image description back to string\n",
        "            \n",
        "            desc = 'startseq ' + ' '.join(image_description) + ' endseq'\n",
        "            descriptions[image_id].append(desc)\n",
        "            \n",
        "    return descriptions\n",
        "\n",
        "# function to load the photo features created using the model\n",
        "def load_photo_features(filename, photos):\n",
        "    \n",
        "    #this will load the entire features\n",
        "    all_features = load(open(filename, 'rb'))\n",
        "    #we are interested in loading the features of the required photos only\n",
        "    features = {k: all_features[k] for k in photos}\n",
        "    \n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKu3JTK86W8X",
        "outputId": "73f5263e-655a-40ee-cbca-a841534c69ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset:  6000\n",
            "Descriptions: train= 6000\n",
            "Photos: train= 6000\n"
          ]
        }
      ],
      "source": [
        "filename = '/content/drive/My Drive/Kaggle/ImageCaptioningGenerator/input/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "\n",
        "train = load_photo_identifiers(filename)\n",
        "print('Dataset: ',len(train))\n",
        "\n",
        "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
        "print('Descriptions: train=', len(train_descriptions))\n",
        "\n",
        "train_features = load_photo_features('/content/drive/My Drive/Kaggle/ImageCaptioningGenerator/features.pkl', train)\n",
        "print('Photos: train=', len(train_features))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHpDc4-a6W8X"
      },
      "source": [
        "To make our model more robust we will reduce our vocabulary to only those words which occur at least 5 times in the entire corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcUvPA5e6W8X",
        "outputId": "37cfc9b4-c053-4f40-e4fb-b9ac2caee34a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary = 1651\n"
          ]
        }
      ],
      "source": [
        "word_count_threshold = 10\n",
        "word_counts = {}\n",
        "nsents = 0\n",
        "for key, sents in train_descriptions.items():\n",
        "    for sent in sents:\n",
        "        nsents += 1\n",
        "        for w in sent.split(' '):\n",
        "            word_counts[w] = word_counts.get(w, 0) + 1\n",
        "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
        "\n",
        "print('Vocabulary = %d' % (len(vocab)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "VZ-9SnHs6W8X"
      },
      "outputs": [],
      "source": [
        "# Dumping the features in a pickle file for further use\n",
        "dump(train, open('train.pkl', 'wb'))\n",
        "\n",
        "# Dumping the features in a pickle file for further use\n",
        "dump(train_descriptions, open('train_descriptions.pkl', 'wb'))\n",
        "\n",
        "# Dumping the features in a pickle file for further use\n",
        "dump(train_features, open('train_features.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM2_gSFI6W8X"
      },
      "source": [
        "The description text will need to be encoded to numbers before it can be presented to the model as in input or compared to the model’s predictions.\n",
        "\n",
        "The first step in encoding the data is to create a consistent mapping from words to unique integer values. Keras provides the Tokenizer class that can learn this mapping from the loaded description data.\n",
        "\n",
        "Below defines the to_lines() to convert the dictionary of descriptions into a list of strings and the create_tokenizer() function that will fit a Tokenizer given the loaded photo description text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Ertwrmw96W8X"
      },
      "outputs": [],
      "source": [
        "# Given the descriptions, fit a tokenizer\n",
        "\n",
        "# TOKENIZER CLASS:\n",
        "# This class allows to vectorize a text corpus, \n",
        "# by turning each text into either a sequence of integers \n",
        "# (each integer being the index of a token in a dictionary) \n",
        "# or, into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf...\n",
        "\n",
        "def create_tokenizer():\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(vocab)\n",
        "    return tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeEN8bmE6W8X",
        "outputId": "908a4f18-aa2b-44c5-aad4-7a7bac40b32a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size:  1652\n"
          ]
        }
      ],
      "source": [
        "tokenizer = create_tokenizer()\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: ', vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEv2mhdj6W8Y"
      },
      "source": [
        "We also need to find out what the max length of a caption can be since we cannot have captions of arbitrary length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "3LHgQolz6W8Y"
      },
      "outputs": [],
      "source": [
        "# convert a dictionary of clean descriptions to a list of descriptions\n",
        "def to_lines(descriptions):\n",
        "    all_desc = list()\n",
        "    for key in descriptions.keys():\n",
        "        [all_desc.append(d) for d in descriptions[key]]\n",
        "    return all_desc\n",
        "\n",
        "#calculated the length of description with most words\n",
        "def max_lengthTEMP(descriptions):\n",
        "    lines = to_lines(descriptions)\n",
        "    return max(len(d.split()) for d in lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L39vD3p_6W8Y"
      },
      "source": [
        "We also need to find out what the max length of a caption can be since we cannot have captions of arbitrary length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELZ0Znxn6W8Y",
        "outputId": "60f9c2ca-1e98-4726-af5c-03d29e32f0d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "max_lengthTEMP(descriptions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO8D6aVd6W8Y"
      },
      "source": [
        "3. Defining the Model\n",
        "\n",
        "Each description will be split into words. The model will be provided one word and the photo and generate the next word. Then the first two words of the description will be provided to the model as input with the image to generate the next word. This is how the model will be trained.\n",
        "\n",
        "The model is in three parts:\n",
        "\n",
        "1. Photo Feature Extractor: This is a Resnet50 model pre-trained on the ImageNet dataset. We have pre-processed the photos with the VGG model (without the output layer) and will use the extracted features predicted by this model as input.\n",
        "\n",
        "2. Sequence Processor: This is a word embedding layer for handling the text input, followed by a Long Short-Term Memory (LSTM) recurrent neural network layer.\n",
        "\n",
        "3. Decoder: Both the feature extractor and sequence processor output a fixed-length vector. These are merged together and processed by a Dense layer to make a final prediction. The Photo Feature Extractor model expects input photo features to be a vector of 4,096 elements. These are processed by a Dense layer to produce a 256 element representation of the photo.\n",
        "\n",
        "The Sequence Processor model expects input sequences with a pre-defined length (34 words) which are fed into an Embedding layer that uses a mask to ignore padded values. This is followed by an LSTM layer with 256 memory units.\n",
        "\n",
        "Both the input models produce a 256 element vector. Further, both input models use regularization in the form of 50% dropout. This is to reduce overfitting the training dataset, as this model configuration learns very fast.\n",
        "\n",
        "The Decoder model merges the vectors from both input models using an addition operation. This is then fed to a Dense 256 neuron layer and then to a final output Dense layer that makes a softmax prediction over the entire output vocabulary for the next word in the sequence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ty77hUp6W8Y"
      },
      "source": [
        "We have to train our model on a lot of images and each image will contain 2048 length feature vector and caption is also represented as numbers. This amount of images is not possible to hold into memory so we will be using a generator method that will yield batches.\n",
        "\n",
        "The generator will yield the input and output sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "6B0LzHCG6W8Z"
      },
      "outputs": [],
      "source": [
        "def create_sequences(tokenizer, max_length, desc_list, photo):\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    # walk through each description for the image\n",
        "    for desc in desc_list:\n",
        "        # encode the sequence\n",
        "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "        # split one sequence into multiple X,y pairs\n",
        "        for i in range(1, len(seq)):\n",
        "            # split into input and output pair\n",
        "            in_seq, out_seq = seq[:i], seq[i]\n",
        "            # pad input sequence\n",
        "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "            # encode output sequence\n",
        "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "            # store\n",
        "            X1.append(photo)\n",
        "            X2.append(in_seq)\n",
        "            y.append(out_seq)\n",
        "    return array(X1), array(X2), array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "zBV94Dke6W8Z"
      },
      "outputs": [],
      "source": [
        "def data_generator(descriptions, photos, tokenizer, max_length):\n",
        "    while 1:\n",
        "        for key, description_list in descriptions.items():\n",
        "            #retrieve photo features\n",
        "            photo = photos[key][0]\n",
        "            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, photo)\n",
        "            yield [[input_image, input_sequence], output_word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "z2oiqh2T6W8Z"
      },
      "outputs": [],
      "source": [
        "# define the captioning model\n",
        "def define_model(vocab_size, max_length):\n",
        "    \n",
        "    # feature extractor model\n",
        "    inputs1 = Input(shape=(2048,))\n",
        "    fe1 = Dropout(0.5)(inputs1)\n",
        "    fe2 = Dense(256, activation='relu')(fe1)\n",
        "\n",
        "    # sequence model\n",
        "    inputs2 = Input(shape=(max_length,))\n",
        "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "    se2 = Dropout(0.5)(se1)\n",
        "    se3 = LSTM(256)(se2)\n",
        "\n",
        "    # decoder model\n",
        "    decoder1 = add([fe2, se3])\n",
        "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "    # tie it together [image, seq] [word]\n",
        "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    \n",
        "    # summarize model\n",
        "    print(model.summary())\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McobQsiQ6W8Z"
      },
      "source": [
        "![Getting Started](./image.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SVUcuZp6W8Z",
        "outputId": "d6604b8e-96c2-4741-d2f1-5a1054326e02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset:  6000\n",
            "Descriptions: train= 6000\n",
            "Photos: train= 6000\n",
            "Vocabulary Size: 1652\n",
            "Description Length:  34\n"
          ]
        }
      ],
      "source": [
        "filename = '/content/drive/My Drive/Kaggle/ImageCaptioningGenerator/input/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "train = load(open('train.pkl', 'rb'))\n",
        "print('Dataset: ', len(train))\n",
        "train_descriptions = load(open('train_descriptions.pkl', 'rb'))\n",
        "print('Descriptions: train=', len(train_descriptions))\n",
        "train_features = load(open('train_features.pkl', 'rb'))\n",
        "print('Photos: train=', len(train_features))\n",
        "tokenizer = create_tokenizer()\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "max_length = max_lengthTEMP(train_descriptions)\n",
        "print('Description Length: ', max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mWKtAEi6W8Z",
        "outputId": "0edcb0c4-2d40-4930-d617-b55e21d600aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)           [(None, 34)]         0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 2048)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 34, 256)      422912      ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 2048)         0           ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 34, 256)      0           ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 256)          524544      ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 256)          525312      ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 256)          0           ['dense[0][0]',                  \n",
            "                                                                  'lstm[0][0]']                   \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 256)          65792       ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 1652)         424564      ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,963,124\n",
            "Trainable params: 1,963,124\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "checkpoint = ModelCheckpoint(filepath='/content/drive/My Drive/Kaggle/ImageCaptioningGenerator/model_{epoch:02d}.h5', \n",
        "                                    save_freq='epoch',\n",
        "                                    monitor='loss',\n",
        "                                    mode='min',\n",
        "                                    save_best_only=True,\n",
        "                                    period = 2)\n",
        "\n",
        "model = define_model(vocab_size, max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDnoxQvlI0zM",
        "outputId": "3e685829-9fe0-4fbd-9fb3-65ddeeb54b4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "6000/6000 [==============================] - 1261s 209ms/step - loss: 2.9013\n",
            "Epoch 2/10\n",
            "6000/6000 [==============================] - 1251s 209ms/step - loss: 2.8790\n",
            "Epoch 3/10\n",
            "6000/6000 [==============================] - 1250s 208ms/step - loss: 2.8730\n",
            "Epoch 4/10\n",
            "6000/6000 [==============================] - 1250s 208ms/step - loss: 2.8673\n",
            "Epoch 5/10\n",
            "6000/6000 [==============================] - 1250s 208ms/step - loss: 2.8648\n",
            "Epoch 6/10\n",
            "6000/6000 [==============================] - 1250s 208ms/step - loss: 2.8656\n",
            "Epoch 7/10\n",
            "6000/6000 [==============================] - 1253s 209ms/step - loss: 2.8668\n",
            "Epoch 8/10\n",
            "6000/6000 [==============================] - 1252s 209ms/step - loss: 2.8652\n",
            "Epoch 9/10\n",
            "6000/6000 [==============================] - 1245s 208ms/step - loss: 2.8652\n",
            "Epoch 10/10\n",
            "6000/6000 [==============================] - 1246s 208ms/step - loss: 2.8637\n"
          ]
        }
      ],
      "source": [
        "epochs = 10\n",
        "steps = len(train_descriptions)\n",
        "generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n",
        "history = model.fit(generator, epochs=epochs, steps_per_epoch=steps, verbose=1, callbacks = checkpoint)\n",
        "model.save('model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "id54eSUN6W8Z"
      },
      "outputs": [],
      "source": [
        "def plot_train (history):\n",
        "    # list all data in history\n",
        "    print(history.history.keys())\n",
        "\n",
        "    # summarize history for loss\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train'], loc='upper left')\n",
        "    plt.show()\n",
        "plot_train(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8q9a9GkB_EK"
      },
      "outputs": [],
      "source": [
        "model = load_model('/content/drive/My Drive/Kaggle/ImageCaptioningGenerator/model_10.h5', compile=False)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ab9Ya0R6W8a"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "dFRBcnPC6W8a"
      },
      "outputs": [],
      "source": [
        "#this function maps an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "#The function below generates a textual description given a trained model, \n",
        "#and a given prepared photo as input. It calls the function word_for_id() \n",
        "#in order to map an integer prediction back to a word.\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "    #start tge generation process\n",
        "    in_text = 'startseq'\n",
        "    #iterating over the max_length since the maximum length of the description can be that only\n",
        "    for i in range(max_length):\n",
        "        #integer ncoding input sequence\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        #padding the input\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        #predicting next word\n",
        "        #the predict function will return probability\n",
        "        prob = model.predict([photo,sequence], verbose=0)\n",
        "        #converting the probability to integer\n",
        "        prob = argmax(prob)\n",
        "        #calling the word_for_id function in order to map integer to word\n",
        "        word = word_for_id(prob, tokenizer)\n",
        "        #breaking if word cannot be mapped\n",
        "        if word is None:\n",
        "            break\n",
        "        #appending as input\n",
        "        in_text += ' ' + word\n",
        "\n",
        "        #break if end is predicted\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    return in_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N8bYDGy6W8a"
      },
      "source": [
        "BLEU, or the Bilingual Evaluation Understudy, is a score for comparing a candidate translation of text to one or more reference translations. The approach works by counting matching n-grams in the candidate translation to n-grams in the reference text. It is common to report the cumulative BLEU-1 to BLEU-4 scores when describing the skill of a text generation system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5UjBjl26W8a"
      },
      "outputs": [],
      "source": [
        "#the below function evaluates the skill of the model\n",
        "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
        "    actual, predicted = list(), list()\n",
        "    for key, desc_list in descriptions.items():\n",
        "        prediction = generate_desc(model, tokenizer, photos[key], max_length)\n",
        "        actual_desc = [d.split() for d in desc_list]\n",
        "        actual.append(actual_desc)\n",
        "        predicted.append(prediction.split())\n",
        "\n",
        "    print('BLEU-1: ', corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "    print('BLEU-2: ', corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "    print('BLEU-3: ', corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "    print('BLEU-4: ', corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "    \n",
        "def max_length(descriptions):\n",
        "    lines = to_lines(descriptions)\n",
        "    return max(len(d.split()) for d in lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGcGqYoc6W8a"
      },
      "outputs": [],
      "source": [
        "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
        "print('Descriptions: train=', len(train_descriptions))\n",
        "tokenizer = create_tokenizer()\n",
        "max_length = max_lengthTEMP(train_descriptions)\n",
        "print('Description Length: ,', max_length)\n",
        "filename = '/content/drive/My Drive/Kaggle/ImageCaptioningGenerator/input/Flickr8k_text/Flickr_8k.testImages.txt'\n",
        "test = load_photo_identifiers(filename)\n",
        "print('Dataset: ', len(test))\n",
        "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
        "print('Descriptions: test=', len(test_descriptions))\n",
        "test_features = load_photo_features('/content/drive/My Drive/Kaggle/ImageCaptioningGenerator/features.pkl', test)\n",
        "print('Photos: test=', len(test_features))\n",
        "filename = '/content/drive/My Drive/Kaggle/ImageCaptioningGenerator/model_10.h5'\n",
        "model = load_model(filename)\n",
        "evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvQQoIE96W8a"
      },
      "source": [
        "# Let's generate a text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "ZhnVLx9k6W8a"
      },
      "outputs": [],
      "source": [
        "def extract_features(filename):\n",
        "    model = ResNet50(include_top=False,weights='imagenet',input_shape=(224,224,3),pooling='avg')\n",
        "    image = load_img(filename, target_size=(224, 224))\n",
        "    image = img_to_array(image)\n",
        "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "    image = preprocess_input(image)\n",
        "    feature = model.predict(image, verbose=0)\n",
        "    return feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uO_LrtUY6W8a",
        "outputId": "abd6afaf-ed29-4229-b097-282353b602c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 16 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f821fde58b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "startseq dog dog running through the the endseq\n"
          ]
        }
      ],
      "source": [
        "model = load_model('/content/drive/My Drive/Kaggle/ImageCaptioningGenerator/model_glove_12.h5')\n",
        "path = '/content/drive/My Drive/Kaggle/ImageCaptioningGenerator/input/Flicker8k_Dataset/96973080_783e375945.jpg'\n",
        "photo = extract_features(path)\n",
        "max_length = 34\n",
        "description = generate_desc(model, tokenizer, photo, max_length)\n",
        "print(description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AK1d0yxI6W8a"
      },
      "outputs": [],
      "source": [
        "import googletrans  \n",
        "from gtts import gTTS\n",
        "import IPython.display as ipd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLKh78xG6W8a"
      },
      "outputs": [],
      "source": [
        "from googletrans import Translator\n",
        "translator = Translator()\n",
        "tranlated = translator.translate(description, dest='ar')\n",
        "print(tranlated.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XfMoGEA6W8b"
      },
      "outputs": [],
      "source": [
        "tts = gTTS(arabic_text, lang='en')\n",
        "tts.save('test.mp3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yJ9zHmt6W8b",
        "outputId": "f0ff433c-a161-42b3-d750-7890a9652562"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
              "                    <source src=\"data:audio/mpeg;base64,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\" type=\"audio/mpeg\" />\n",
              "                    Your browser does not support the audio element.\n",
              "                </audio>\n",
              "              "
            ],
            "text/plain": [
              "<IPython.lib.display.Audio object>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "audio_path=\"test.mp3\"\n",
        "\n",
        "ipd.Audio(audio_path, autoplay=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9Nk3cwS6W8b"
      },
      "source": [
        "# using glove as a pre trained embedding model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO_fdBan6W8b"
      },
      "source": [
        "To encode our text sequence we will map every word to a 200-dimensional vector. For this will use a pre-trained Glove model. This mapping will be done in a separate layer after the input layer called the embedding layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssYRqj1S6W8b"
      },
      "source": [
        "To generate the caption we will be using two popular methods which are Greedy Search and Beam Search. These methods will help us in picking the best words to accurately define the image.\n",
        "\n",
        "The basic premise behind Glove is that we can derive semantic relationships between words from the co-occurrence matrix. For our model, we will map all the words in our max_length-word long caption to a 200-dimension vector using Glove.\n",
        "\n",
        "The advantage of using Glove over Word2Vec is that GloVe does not just rely on the local context of words but it incorporates global word co-occurrence to obtain word vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIrWcwCq6W8b"
      },
      "outputs": [],
      "source": [
        "ixtoword = {}\n",
        "wordtoix = {}\n",
        "ix = 1\n",
        "for w in vocab:\n",
        "    wordtoix[w] = ix\n",
        "    ixtoword[ix] = w\n",
        "    ix += 1\n",
        "\n",
        "vocab_size = len(ixtoword) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "DbZVNXyZ6W8c"
      },
      "outputs": [],
      "source": [
        "glove_path = '/content/drive/My Drive/Kaggle/ImageCaptioningGenerator/input/glove6b'\n",
        "embeddings_index = {} \n",
        "with open('/content/drive/My Drive/Kaggle/ImageCaptioningGenerator/input/glove6b/glove.6B.200d.txt', 'rb') as f:\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "CE17pZMn6W8c"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 200\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "YhsJ6Ucx6W8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33fd26a5-8611-40bb-da5d-31aa0b0d11b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_9 (InputLayer)           [(None, 34)]         0           []                               \n",
            "                                                                                                  \n",
            " input_8 (InputLayer)           [(None, 2048)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, 34, 200)      330400      ['input_9[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 2048)         0           ['input_8[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 34, 200)      0           ['embedding_2[0][0]']            \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 256)          524544      ['dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  (None, 256)          467968      ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 256)          0           ['dense_6[0][0]',                \n",
            "                                                                  'lstm_2[0][0]']                 \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 256)          65792       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 1652)         424564      ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,813,268\n",
            "Trainable params: 1,813,268\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "inputs1 = Input(shape=(2048,))\n",
        "fe1 = Dropout(0.5)(inputs1)\n",
        "fe2 = Dense(256, activation='relu')(fe1)\n",
        "\n",
        "inputs2 = Input(shape=(max_length,))\n",
        "se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.5)(se1)\n",
        "se3 = LSTM(256)(se2)\n",
        "\n",
        "decoder1 = add([fe2, se3])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRjUkcbE6W8c"
      },
      "source": [
        "Before training the model we need to keep in mind that we do not want to retrain the weights in our embedding layer (pre-trained Glove vectors)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "AFjy3S_R6W8c"
      },
      "outputs": [],
      "source": [
        "model.layers[2].set_weights([embedding_matrix])\n",
        "model.layers[2].trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "OiD2QHFw6W8c"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator(descriptions, photos, tokenizer, max_length, num_photos_per_batch):\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    n=0\n",
        "    # loop for ever over images\n",
        "    while 1:\n",
        "        for key, desc_list in descriptions.items():\n",
        "            n+=1\n",
        "            # retrieve the photo feature\n",
        "            photo = photos[key][0]\n",
        "            for desc in desc_list:\n",
        "                # encode the sequence\n",
        "                seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "                # split one sequence into multiple X, y pairs\n",
        "                for i in range(1, len(seq)):\n",
        "                    # split into input and output pair\n",
        "                    in_seq, out_seq = seq[:i], seq[i]\n",
        "                    # pad input sequence\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                    # encode output sequence\n",
        "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "                    # store\n",
        "                    X1.append(photo)\n",
        "                    X2.append(in_seq)\n",
        "                    y.append(out_seq)\n",
        "\n",
        "            if n==num_photos_per_batch:\n",
        "                yield ([array(X1), array(X2)], array(y))\n",
        "                X1, X2, y = list(), list(), list()\n",
        "                n=0"
      ],
      "metadata": {
        "id": "4t64q4adv0b9"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0lCN8J66W8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38270e82-8923-4a96-c37a-cb32203c0631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "2000/2000 [==============================] - 783s 390ms/step - loss: 4.1529\n",
            "Epoch 2/30\n",
            "2000/2000 [==============================] - 751s 376ms/step - loss: 4.1436\n",
            "Epoch 3/30\n",
            "2000/2000 [==============================] - 739s 370ms/step - loss: 4.1385\n",
            "Epoch 4/30\n",
            " 448/2000 [=====>........................] - ETA: 10:03 - loss: 4.0983"
          ]
        }
      ],
      "source": [
        "epochs = 30\n",
        "batch_size = 3\n",
        "steps = len(train_descriptions)//batch_size\n",
        "checkpoint = ModelCheckpoint(filepath='/content/drive/My Drive/Kaggle/ImageCaptioningGenerator/model_glove1_{epoch:02d}.h5', \n",
        "                                    save_freq='epoch',\n",
        "                                    monitor='loss',\n",
        "                                    mode='min',\n",
        "                                    save_best_only=True,\n",
        "                                    period = 2)\n",
        "\n",
        "generator = data_generator(train_descriptions, train_features, tokenizer, max_length, batch_size)\n",
        "model.fit(generator, epochs=epochs, steps_per_epoch=steps, verbose=1, callbacks = checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('/content/drive/My Drive/Kaggle/ImageCaptioningGenerator/model_glove_12.h5', compile=False)"
      ],
      "metadata": {
        "id": "izC1vxCvh21d"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QivFlQZO6W8d"
      },
      "source": [
        "# Greedy and Beam Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYmIQzvp6W8d"
      },
      "source": [
        "As the model generates a 1660 long vector with a probability distribution across all the words in the vocabulary we greedily pick the word with the highest probability to get the next word prediction. This method is called Greedy Search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjQPCGdI6W8d"
      },
      "outputs": [],
      "source": [
        "def greedySearch(photo):\n",
        "    in_text = 'startseq'\n",
        "    for i in range(max_length):\n",
        "        seq = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        yhat = model.predict([photo,sequence], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        word = word_for_id[yhat]\n",
        "        in_text += ' ' + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "\n",
        "    final = in_text.split()\n",
        "    final = final[1:-1]\n",
        "    final = ' '.join(final)\n",
        "    return final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rumpf-K-6W8d"
      },
      "source": [
        "Beam Search is where we take top k predictions, feed them again in the model and then sort them using the probabilities returned by the model. So, the list will always contain the top k predictions and we take the one with the highest probability and go through it till we encounter ‘endseq’ or reach the maximum caption length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF-lAnuy6W8d"
      },
      "outputs": [],
      "source": [
        "def beam_search_predictions(image, beam_index = 3):\n",
        "    start = [word_for_id[\"startseq\"]]\n",
        "    start_word = [[start, 0.0]]\n",
        "    while len(start_word[0][0]) < max_length:\n",
        "        temp = []\n",
        "        for s in start_word:\n",
        "            par_caps = sequence.pad_sequences([s[0]], maxlen=max_length, padding='post')\n",
        "            preds = model.predict([image,par_caps], verbose=0)\n",
        "            word_preds = np.argsort(preds[0])[-beam_index:]\n",
        "            # Getting the top <beam_index>(n) predictions and creating a \n",
        "            # new list so as to put them via the model again\n",
        "            for w in word_preds:\n",
        "                next_cap, prob = s[0][:], s[1]\n",
        "                next_cap.append(w)\n",
        "                prob += preds[0][w]\n",
        "                temp.append([next_cap, prob])\n",
        "                    \n",
        "        start_word = temp\n",
        "        # Sorting according to the probabilities\n",
        "        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n",
        "        # Getting the top words\n",
        "        start_word = start_word[-beam_index:]\n",
        "    \n",
        "    start_word = start_word[-1][0]\n",
        "    intermediate_caption = [word_for_id[i] for i in start_word]\n",
        "    final_caption = []\n",
        "    \n",
        "    for i in intermediate_caption:\n",
        "        if i != 'endseq':\n",
        "            final_caption.append(i)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    final_caption = ' '.join(final_caption[1:])\n",
        "    return final_caption"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRTG9dsE6W8d"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-stxyIi6W8d"
      },
      "outputs": [],
      "source": [
        "print(\"Greedy Search:\",greedySearch(image))\n",
        "print(\"Beam Search, K = 3:\",beam_search_predictions(image, beam_index = 3))\n",
        "print(\"Beam Search, K = 5:\",beam_search_predictions(image, beam_index = 5))\n",
        "print(\"Beam Search, K = 7:\",beam_search_predictions(image, beam_index = 7))\n",
        "print(\"Beam Search, K = 10:\",beam_search_predictions(image, beam_index = 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMkd1uVo6W8d"
      },
      "source": [
        "Things you can implement to improve your model:-\n",
        "\n",
        "- Make use of the larger datasets, especially the MS COCO dataset or the Stock3M dataset which is 26 times larger than MS COCO.\n",
        "- Implementing an Attention Based model:- Attention-based mechanisms are becoming increasingly popular in deep learning because they can dynamically focus on the various parts of the input image while the output sequences are being produced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0XVNq-F6W8d"
      },
      "source": [
        "https://www.analyticsvidhya.com/blog/2020/11/create-your-own-image-caption-generator-using-keras/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-l5pG1nr6W8e"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "aeae005699da9ee22f5f1999a0228188e87e61d9f977527a9e2e028c3963d1b8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}